# üëÄ Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion

[**üìñ arXiv**](https://arxiv.org/pdf/2402.12195.pdf) | [**ü§ó Models**](coming soon)

This repo includes codes and examples for paper [Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion](https://arxiv.org/pdf/2402.12195.pdf). 

## Framework
We propose a paradigm **Bro**wse and Concentra**te** (**Brote**) for incorporating multimodal context before feeding features into the LLM, together with two approaches to implement our paradigm, Brote-EX and Brote-IM. The model structures are shown in the following figure.
<img src="./figures/model.png" alt="Image" width="400">
![model](./figures/model.png)

## Installation
coming soon

## Instructions For Training and Inference
coming soon

## Example
![example](./figures/git_showcase.png)

(üê± in this figure is a 6-year-old cat, his name is Alan.)

## Models
coming soon

## Reference

üìë If you find our project helpful to your research, please consider citing:
```
@article{wang2024browse,
  title={Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion},
  author={Wang, Ziyue and Chen, Chi and Zhu, Yiqi and Luo, Fuwen and Li, Peng and Yan, Ming and Zhang, Ji and Huang, Fei and Sun, Maosong and Liu, Yang},
  journal={arXiv preprint arXiv:2402.12195},
  year={2024}
}
```
